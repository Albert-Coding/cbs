#!/bin/bash

# setup environment
export MYDIR="$(dirname $(dirname $0))"
export RUNBIN="${HYDRA_RUN}/bin"
export PIDDIR="${HYDRA_RUN}/etc/pid"
export HYDRA_JAR="${MYDIR}/lib/hydra.jar"
export HYDRA_HOST="${HYDRA_HOST:-$(hostname)}"
export PATH="${PATH}:${RUNBIN}"

exit=0
# check dependencies
[ -z "${HYDRA_RUN}" ] && echo "set HYDRA_RUN environment variable" && exit=1
[ ! $(which java) ] && echo "missing java executable" && exit=1
[ ! $(which rabbitmq-server) ] && echo "missing rabbitmq-server executable" && exit=1
[ ! -d ${HYDRA_RUN} ] && echo "missing 'run' directory. aborting." && exit=1
[ ! -f ${HYDRA_JAR} ] && echo "missing hydra exec jar @ ${HYDRA_JAR}" && exit=1
# exit if missing any dependency
[ ${exit} -gt 0 ] && exit ${exit}

export QUERY_OPT="-Ddebug.MeshQuerySource=2 -Dmesh.local.handlers=com.addthis.hydra.data.query.source.MeshQuerySource -Dmqmaster.mesh.peers=localhost -Dmeshy.senders=1 -Dmeshy.stream.prefetch=true -Dmqmaster.mesh.peer.port=5101 -Dmesh.query.root=minion -Dquery.tmpdir=log/query/tmp"
export QMASTER_OPT="${QUERY_OPT} -Xmx512M -Xms512M -Deps.mem.debug=10000 -Dbatch.job.log4j=1 -Dcs.je.cacheShared=1 -Dcs.je.deferredWrite=1 -Dhydra.query.debug=1 -Dcom.yammer.metrics.GcMonitor.enable=false -Ddebug.level.SourceTracker=1 -Ddebug.level.QueryCache=2 -Ddebug.level.MeshQueryMaster=2 -Dquery.cache.enable=false -DQueryCache.CACHE_DIR=log/query/cache -Dmqmaster.enableZooKeeper=true"
export QWORKER_OPT="${QUERY_OPT} -Xmx256M -Xms256M"
export MINION_OPT="-Xmx128M -Xms128M -Dminion.mem=512 -Dminion.localhost=${HYDRA_HOST} -Dminion.group=local -Dminion.web.port=7070 -Dspawn.localhost=localhost -Dhttp.post.max=327680 -Dminion.sparse.updates=1 -Dreplicate.cmd.delay.seconds=1 -Dbackup.cmd.delay.seconds=0"
export SPAWN_OPT="-Xmx128M -Xms128M -Dspawn.localhost=localhost -Dspawn.queryhost=${HYDRA_HOST} -Dspawn.status.interval=6000 -Dspawn.chore.interval=3000 -Dhttp.post.max=327680 -Dspawn.polltime=10000 -Dspawnbalance.min.disk.percent.avail.replicas=0.01 -Dspawn.auth.ldap=false -Dmesh.port=5000 -Djob.store.remote=false"
export MESH_OPT="-Xmx128M -Xms128M -Dmeshy.autoMesh=false -Dmeshy.throttleLog=true -Dmeshy.buffers.enable=true -Dmeshy.stream.maxopen=10000 -Ddebug.level.StreamService=0 -Ddebug.level.ChannelState=0"
export COMMON_OPT="-Dzk.servers=localhost:2181 -Dganglia.enable=false -Djava.net.preferIPv4Stack=true"
export HYDRA_OPT="-Dbatch.brokerHost=localhost -Dbatch.brokerPort=5672 -Dhydra.tree.cache.maxSize=250 -Dhydra.tree.page.maxSize=50 -Deps.mem.debug=0"
export LOG4J_OPT="-Dlog4j.defaultInitOverride=true -Dlog4j.configuration=${HYDRA_RUN}/etc/log4j.conf"
export JAVA_CMD="java -server ${JAVA_OPTS} ${COMMON_OPT} ${EXTOPT}"
export HYDRA_CMD="${JAVA_CMD} ${LOG4J_OPT} ${HYDRA_OPT}"
export HYDRA_EXE="-jar ${HYDRA_JAR}"

# do in scope of HYDRA_RUN
(
	cd ${HYDRA_RUN}
	# create missing directories
	for dir in bin lib log log/kafka log/query etc etc/pid etc/zookeeper mesh minion; do
		[ ! -d $dir ] && mkdir -p $dir && echo "created '${dir}'"
	done
	# link to default scripts and directories
	[ ! -h web ] && ln -s ${MYDIR}/web/ web
	(
		cd mesh
		rm -f job
		ln -s ../minion/ job
		[ ! -h log ] && ln -s ../log/ log
	)
	[ ! -f bin/job-task.sh ] && ln -s ${MYDIR}/lib/job-task.sh bin/job-task.sh
	[ ! -f etc/log4j.conf ] && ln -s ${MYDIR}/etc/log4j.conf etc/log4j.conf
	[ ! -f etc/log4j-quiet.conf ] && ln -s ${MYDIR}/etc/log4j-quiet.conf etc/log4j-quiet.conf
	[ ! -f etc/kafka-broker.properties ] && ln -s ${MYDIR}/etc/kafka-broker.properties etc/kafka-broker.properties
	[ ! -f etc/zookeeper.properties ] && ln -s ${MYDIR}/etc/zookeeper.properties etc/zookeeper.properties
)

# pull in hydra properties if they exist
if [ -f ${HYDRA_RUN}/etc/hydra.properties ]; then
	CMD_ADD=$(grep = ${HYDRA_RUN}/etc/hydra.properties | while read one; do echo "-D${one}"; done)
	export JAVA_CMD="${JAVA_CMD} ${CMD_ADD}"
	export HYDRA_CMD="${HYDRA_CMD} ${CMD_ADD}"
fi

# flcow (copy-on-write) support for 64 bit linux & os x
case $(uname) in
	Linux)
		export LD_PRELOAD=${MYDIR}/lib/libflcow.so:${LD_PRELOAD}
		;;
	Darwin)
		export LD_PRELOAD=${MYDIR}/lib/libflcow.dylib:${LD_PRELOAD}
		;;
esac
export FLCOW_PATH="^${HYDRA_RUN}"
export FLCOW_EXCLUDE="\.((stats)|(pid)|(done)|(complete))$"

function startProcess() {
	export PROCNAME=$1
	export SLEEP=${2:-0}
	cd ${HYDRA_RUN}
	PID="${PIDDIR}/pid.${PROCNAME}"
	LOG=log/${PROCNAME}.log
	if [ ! -f "${PID}" ]; then
		case ${PROCNAME} in
			meshy) ${HYDRA_CMD} ${MESH_OPT} ${HYDRA_EXE} mesh server 5000 mesh > ${LOG} 2>&1 & ;;
			rabbit) nohup rabbitmq-server > ${LOG} 2>&1 & ;;
			zookeeper) ${JAVA_CMD} -cp ${HYDRA_JAR} org.apache.zookeeper.server.quorum.QuorumPeerMain ./etc/zookeeper.properties > ${LOG} 2>&1 & ;;
			mqmaster) ${HYDRA_CMD} ${QMASTER_OPT} ${HYDRA_EXE} mqmaster etc web jar > ${LOG} 2>&1 & ;;
			mqworker) ${HYDRA_CMD} ${QWORKER_OPT} ${HYDRA_EXE} mqworker server 5101 ${HYDRA_RUN} ${HYDRA_HOST}:5100 > ${LOG} 2>&1 & ;;
			spawn) ${HYDRA_CMD} ${SPAWN_OPT} ${HYDRA_EXE} spawn etc web > ${LOG} 2>&1 & ;;
			minion)
				if [ ! -f minion/minion.state ]; then
					mkdir -p minion
					echo "creating default minion state"
					echo "{stopped:{},uuid:\"minion\"}" > minion/minion.state
				fi
				echo "${HYDRA_CMD} ${MINION_OPT} ${HYDRA_EXE} minion minion" > log/minion.cmd
				${HYDRA_CMD} ${MINION_OPT} ${HYDRA_EXE} minion minion > ${LOG} 2>&1 &
				;;
			default) echo "unknown process: ${PROCNAME}"; return;
		esac
		echo "$!" > "${PID}"
		echo "started ${PROCNAME}"
		sleep ${SLEEP}
	fi
}

function stopProcess() {
	cd ${HYDRA_RUN}
	for process in $*; do
		if [ -f ${PIDDIR}/pid.${process} ]; then
			PID=$(cat ${PIDDIR}/pid.${process})
			tries=1
			if [ -d /proc ]; then
				# in linux, use /proc filesystem. yay
				while [ -d /proc/${PID} ]; do
					echo "stopping ${process} #${tries}"
					[ ${tries} -gt 5 ] && SIG="-9"
					kill ${SIG} ${PID} # 2>/dev/null
					[ ${tries} -gt 1 ] && sleep 1 || usleep 250000
					tries=$((tries+1))
				done
			else
				# everywhere else, use ps. booo
				SIG=""
				EXIST="$(ps ax | grep -v grep | grep ${PID})" 
				while [ "$(ps a | grep -v grep | grep ${PID})" != "" ]; do
					echo "stopping ${process} #${tries} @ ${PID}"
					[ ${tries} -gt 5 ] && SIG="-9"
					kill ${SIG} ${PID} 2>/dev/null || break
					[ ${tries} -gt 1 ] && sleep 1 || sleep 1
					tries=$((tries+1))
				done
			fi
			rm ${PIDDIR}/pid.${process}
		fi
	done

}

case $1 in
	exe)
		shift
		${HYDRA_CMD} ${HYDRA_EXE} $*
		exit
		;;
	query)
		shift
		${HYDRA_CMD} -Dlog4j.configuration=${HYDRA_RUN}/etc/log4j-quiet.conf ${HYDRA_EXE} qutil $*
		exit
		;;
	mesh)
		shift
		${HYDRA_CMD} -Dlog4j.configuration=${HYDRA_RUN}/etc/log4j-quiet.conf ${HYDRA_EXE} mesh client $*
		exit
		;;
esac

case "$1-$2" in
	local-start)
		startProcess meshy
		startProcess rabbit
		startProcess zookeeper
		startProcess mqmaster
		startProcess spawn
		startProcess mqworker
		startProcess minion
		;;
	local-stop)
		stopProcess mqworker minion kafka
		stopProcess mqmaster
		stopProcess spawn
		stopProcess zookeeper
		[ -f ${PIDDIR}/pid.rabbitmq ] && echo "stopping rabbitmq" && rm ${PIDDIR}/pid.rabbitmq && rabbitmqctl stop
		stopProcess meshy
		;;
	local-restart)
		hcl local stop
		sleep 1
		hcl local start
		;;
	local-seed)
		curl 'http://localhost:5050/command.put' --data 'label=default-task&owner=install&cpu=1&mem=512&io=1&command=${RUNBIN}/job-task.sh job.conf {{nodes}} {{node}} {{jobid}}'
		echo "created default spawn command"
		;;
	job-list)
		job=${3:-nojob}
		(cd ${HYDRA_RUN}/minion/; find * -maxdepth 0 -type d)
		;;
	job-tail)
		job=${3:-nojob}
		tail -f ${HYDRA_RUN}/minion/${job}/*/live/log/log.{out,err}
		;;
	job-clean)
		job=${3:-nojob}
		rm -rf ${HYDRA_RUN}/minion/${job}/*/*
		;;
	job-cleanrun)
		job=${3:-nojob}
		hcl job clean ${job}
		hcl job enable ${job}
		hcl job kick ${job}
		sleep 3
		hcl job tail ${job}
		;;
	job-kick)
		host=${4:-${HYDRA_HOST}:5050}
		job=${3:-nojob}
		curl "http://${host}/job.submit?id=${job}&spawn=1"; echo
		;;
	job-enable)
		host=${4:-${HYDRA_HOST}:5050}
		job=${3:-nojob}
		curl "http://${host}/job.set?id=${job}&enable=1"; echo
		;;
	job-delete)
		host=${4:-${HYDRA_HOST}:5050}
		job=${3:-nojob}
		curl "http://${host}/job.delete?id=${job}"; echo
		;;
	push-job)
		shift;shift
		if [ -z "${1}" ]; then
			echo "usage: push-job <conf> <jobid> [host:port]"
			exit
		fi
		conf=${1:-job.conf}
		job=${2:-nojobid}
		host=${3:-${HYDRA_HOST}:5050}
		echo -n "posting job update (conf=$conf job=$job host=$host) ... "
		/bin/echo -n "id=${job}&field=config&value=" > /tmp/tmp.post
		cat ${conf} | sed 's/%/%25/g' | sed 's/&/%26/g' >> /tmp/tmp.post
		curl  --data-binary @/tmp/tmp.post "http://${host}/job.set"
		echo
		;;
	push-macro)
		shift;shift
		if [ -z ${1} ]; then
			echo "usage: push-macro <macro_file> [host:port] [macro_id] [owner] [description]"
			exit
		fi
		macro_file=${1}
		host=${2:-${HYDRA_HOST}:5050}
		macro_id=${3}
		owner=${4}
		desc=${5}
		if [ -z ${macro_file} ]; then
			echo "missing macro file"
			exit
		fi
		if [ -z ${macro_id} ]; then
			macro_id=$(grep "// id:" ${macro_file} | while read a b c; do echo "$c"; done)
		fi
		if [ -z ${macro_id} ]; then
			echo "missing macro id in $macro_file"
			exit
		fi
		if [ -z ${owner} ]; then
			owner=$(grep "// owner:" ${macro_file} | while read a b c; do echo "$c"; done)
		fi
		if [ -z ${desc} ]; then
			desc=$(grep "// description:" ${macro_file} | while read a b c; do echo "$c"; done)
		fi
		echo -n "pushing macro file=${macro_file} id=${macro_id} owner=${owner} desc=${description} host=${host} ... "
		/bin/echo -n "label=${macro_id}&owner=${owner}&description=${desc}&macro=" > /tmp/tmp.post
		cat ${macro_file} | sed 's/%/%25/g' | sed 's/&/%26/g' | sed 's/+/%2b/g' >> /tmp/tmp.post
		curl --data-binary @/tmp/tmp.post "http://${host}/macro.put"
		echo
		;;
	*)
		cat << EOF
commands: 
  exe    [ args ]                     -- invoke hydra jar directly
  job    [ kick | tail | list ]       -- simple job control / inspection
  job    [ clean | delete | enable ]  -- hollow out, delete or enable a job
  job    [ cleanrun ]                 -- hollow out then run a job
  local  [ start | stop | seed ]      -- start/stop complete local stack
  mesh   <host> <port> <command>      -- mesh client command
  push   [ job | macro ]              -- inject job or macro into spawn
  query  [ args ]                     -- query local or remote map data
EOF
		;;
esac

